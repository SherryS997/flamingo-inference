{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Flamingo Inference Notebook\n",
    "\n",
    "This notebook demonstrates how to use the Audio Flamingo model for inference on audio files with various prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from transformers import AutoTokenizer, set_seed\n",
    "from google.colab import files\n",
    "\n",
    "# You'll need to implement these imports based on your actual project structure\n",
    "# from src.factory import create_model_and_transforms\n",
    "# from data import AudioTextDataProcessor\n",
    "\n",
    "set_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your config file\n",
    "uploaded = files.upload()\n",
    "config_file = next(iter(uploaded))\n",
    "\n",
    "# Load configuration\n",
    "with open(config_file, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "clap_config = config['clap_config']\n",
    "model_config = config['model_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer(model_config):\n",
    "    tokenizer_path = model_config['tokenizer_path']\n",
    "    cache_dir = model_config['cache_dir']\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_path,\n",
    "        local_files_only=False,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    text_tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"<audio>\", \"<|endofchunk|>\"]}\n",
    "    )\n",
    "    if text_tokenizer.pad_token is None:\n",
    "        text_tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "    if text_tokenizer.sep_token is None:\n",
    "        text_tokenizer.add_special_tokens({\"sep_token\": \"<SEP>\"})\n",
    "    return text_tokenizer\n",
    "\n",
    "tokenizer = prepare_tokenizer(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your model checkpoint\n",
    "uploaded = files.upload()\n",
    "checkpoint_path = next(iter(uploaded))\n",
    "\n",
    "def prepare_model(model_config, clap_config, checkpoint_path):\n",
    "    model, _ = create_model_and_transforms(\n",
    "        **model_config,\n",
    "        clap_config=clap_config,\n",
    "        use_local_files=False,\n",
    "        gradient_checkpointing=False,\n",
    "        freeze_lm_embeddings=False,\n",
    "    )\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    model_state_dict = checkpoint[\"model_state_dict\"]\n",
    "    model_state_dict = {k.replace(\"module.\", \"\"): v for k, v in model_state_dict.items()}\n",
    "    model.load_state_dict(model_state_dict, False)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = prepare_model(model_config, clap_config, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data processor\n",
    "DataProcessor = AudioTextDataProcessor(\n",
    "    data_root='/home/sherry/Code/flamingo-inference/model_ckpts/datasets',\n",
    "    clap_config=clap_config,\n",
    "    tokenizer=tokenizer,\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, tokenizer, item, processed_item):\n",
    "    filename, audio_clips, audio_embed_mask, input_ids, attention_mask = processed_item\n",
    "    audio_clips = audio_clips.to(device, dtype=None, non_blocking=True)\n",
    "    audio_embed_mask = audio_embed_mask.to(device, dtype=None, non_blocking=True)\n",
    "    input_ids = input_ids.to(device, dtype=None, non_blocking=True).squeeze()\n",
    "\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    inference_kwargs = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"num_return_sequences\": 1\n",
    "    }\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        audio_x=audio_clips.unsqueeze(0),\n",
    "        audio_x_mask=audio_embed_mask.unsqueeze(0),\n",
    "        lang_x=input_ids.unsqueeze(0),\n",
    "        eos_token_id=eos_token_id,\n",
    "        max_new_tokens=128,\n",
    "        **inference_kwargs,\n",
    "    )\n",
    "\n",
    "    outputs_decoded = [\n",
    "        tokenizer.decode(output).split(tokenizer.sep_token)[-1].replace(tokenizer.eos_token, '').replace(tokenizer.pad_token, '').replace('<|endofchunk|>', '') for output in outputs\n",
    "    ]\n",
    "\n",
    "    return outputs_decoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio_file, prompt):\n",
    "    item = {\n",
    "        'name': audio_file,\n",
    "        'prefix': \"The task is audio analysis.\",\n",
    "        'prompt': prompt\n",
    "    }\n",
    "    processed_item = DataProcessor.process(item)\n",
    "    response = inference(model, tokenizer, item, processed_item)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload an audio file\n",
    "uploaded = files.upload()\n",
    "audio_file = next(iter(uploaded))\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Describe the sound in this audio file.\"\n",
    "response = process_audio(audio_file, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive cell for trying different prompts\n",
    "while True:\n",
    "    prompt = input(\"Enter your prompt (or 'q' to quit): \")\n",
    "    if prompt.lower() == 'q':\n",
    "        break\n",
    "    response = process_audio(audio_file, prompt)\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
